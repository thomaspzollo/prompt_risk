{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "46b3a125-db97-4ec7-a476-cb92004a51ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "accc406f-051e-457d-8463-5d558752b7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bj_bounds = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d880f1d4-a0b0-44ff-8d5f-e06ae0970de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binedges_equalmass(x, n_bins):\n",
    "    n = len(x)\n",
    "    return np.interp(np.linspace(0, n, n_bins + 1),\n",
    "                     np.arange(n),\n",
    "                     np.sort(x))\n",
    "\n",
    "def find_bin_edges_equal_mass_src(source_data, n_bins, clf):\n",
    "    ## compute iw using training set\n",
    "    w_list_train = []\n",
    "    X_source = source_data[0]\n",
    "    w_source = 1/clf.predict_proba(X_source)[:, 1]-1\n",
    "\n",
    "    bin_edges = binedges_equalmass(list(w_source), n_bins)\n",
    "    bin_edges[0] = 0.0\n",
    "    bin_edges[-1] = np.inf\n",
    "    return bin_edges\n",
    "\n",
    "def bci_clopper_pearson(k, n, alpha, two_side=True, use_R=False):\n",
    "    if two_side:\n",
    "        if use_R: # R is numerically better when alpha is small\n",
    "            from rpy2.robjects.packages import importr\n",
    "            stats = importr('stats')\n",
    "\n",
    "            lo = stats.qbeta(alpha/2, int(k), int(n-k+1))[0]\n",
    "            hi = stats.qbeta(1 - alpha/2, int(k+1), int(n-k))[0]\n",
    "        else:\n",
    "            from scipy import stats\n",
    "\n",
    "            lo = stats.beta.ppf(alpha/2, k, n-k+1)\n",
    "            hi = stats.beta.ppf(1 - alpha/2, k+1, n-k)\n",
    "        \n",
    "            lo = 0.0 if math.isnan(lo) else lo\n",
    "            hi = 1.0 if math.isnan(hi) else hi\n",
    "    \n",
    "        return lo, hi\n",
    "    else:\n",
    "        if use_R: # R is numerically better when alpha is small\n",
    "            from rpy2.robjects.packages import importr\n",
    "            stats = importr('stats')\n",
    "\n",
    "            hi = stats.qbeta(1 - alpha, int(k+1), int(n-k))[0]\n",
    "        else:\n",
    "            from scipy import stats\n",
    "\n",
    "            hi = stats.beta.ppf(1 - alpha, k+1, n-k)\n",
    "            hi = 1.0 if math.isnan(hi) else hi\n",
    "    \n",
    "        return hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9569252c-ab8a-4b81-a624-8cd99734b1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32007/955575122.py:3: DtypeWarning: Columns (22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  source_df = pd.read_csv(filepath)\n"
     ]
    }
   ],
   "source": [
    "chat_root = \"full\"\n",
    "filepath = \"../llm_output/{}_chat/google-flan-t5-xxl_predictions.csv\".format(chat_root)\n",
    "source_df = pd.read_csv(filepath)\n",
    "\n",
    "test_hyp = None\n",
    "for h in source_df[\"hypothesis\"].unique():\n",
    "    rows = source_df[source_df[\"hypothesis\"] == h]\n",
    "    if len(rows) == 8000:\n",
    "        test_hyp = h\n",
    "        break\n",
    "source_df = source_df[source_df[\"hypothesis\"] == test_hyp]\n",
    "source_df = source_df.sort_values([\"task_id\"])\n",
    "source_tasks = source_df[\"task_id\"].tolist()\n",
    "\n",
    "source_emb_df = pd.read_csv(\"../llm_output/{}_chat/sentence-transformers-multi-qa-mpnet-base-dot-v1_embeddings.csv\".format(chat_root))\n",
    "source_emb_df = source_emb_df.sort_values([\"task_id\"])\n",
    "source_emb_tasks = source_emb_df[\"task_id\"].tolist()\n",
    "\n",
    "assert source_tasks == source_emb_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2747df38-d7c1-46f2-9e19-886967192805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 768), (8000,))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_source = np.array([eval(s) for s in source_emb_df[\"embedding\"].tolist()])\n",
    "z_source = np.array(source_df[\"toxicity\"].tolist())\n",
    "\n",
    "X_source.shape, z_source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a50af2-7d3b-48e1-9c1d-32d5a101a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_root = \"red_team\"\n",
    "filepath = \"../llm_output/{}_chat/google-flan-t5-xxl_predictions.csv\".format(chat_root)\n",
    "target_df = pd.read_csv(filepath)\n",
    "\n",
    "target_df = target_df[target_df[\"hypothesis\"] == test_hyp]\n",
    "target_df = target_df.sort_values([\"task_id\"])\n",
    "target_tasks = target_df[\"task_id\"].tolist()\n",
    "\n",
    "target_emb_df = pd.read_csv(\"../llm_output/{}_chat/sentence-transformers-multi-qa-mpnet-base-dot-v1_embeddings.csv\".format(chat_root))\n",
    "target_emb_df = target_emb_df.sort_values([\"task_id\"])\n",
    "target_emb_tasks = target_emb_df[\"task_id\"].tolist()\n",
    "\n",
    "assert target_tasks == target_emb_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942557d3-7225-4d4f-a3ce-a7c3224c351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_target = np.array([eval(s) for s in target_emb_df[\"embedding\"].tolist()])\n",
    "z_target = np.array(target_df[\"toxicity\"].tolist())\n",
    "\n",
    "X_target.shape, z_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53207f81-a228-4875-abb4-44f02f688552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e36659-dd1f-4a2c-bfb3-52a15686cb1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c4a057-6658-409f-aaa3-d56d61525606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c09e9e26-8855-4bfa-a367-820ce0f0cb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32007/1630692288.py:6: DtypeWarning: Columns (21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  chat_df = pd.read_csv(os.path.join(chat_dir, 'google-flan-t5-xxl_predictions.csv'))\n",
      "/tmp/ipykernel_32007/1630692288.py:19: DtypeWarning: Columns (21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  chat_df = pd.read_csv(os.path.join(chat_dir, 'google-flan-t5-xxl_predictions.csv'))\n"
     ]
    }
   ],
   "source": [
    "output_dir = '../llm_output'\n",
    "\n",
    "chat_root = \"full\"\n",
    "chat_dir = os.path.join(output_dir, \"{}_chat\".format(chat_root))\n",
    "\n",
    "chat_df = pd.read_csv(os.path.join(chat_dir, 'google-flan-t5-xxl_predictions.csv'))\n",
    "chat_df = chat_df.sort_values([\"task_id\"])\n",
    "chat_tasks = sorted(chat_df[\"task_id\"].unique())\n",
    "\n",
    "source_df = pd.read_csv(os.path.join(chat_dir, 'sentence-transformers-multi-qa-mpnet-base-dot-v1_embeddings.csv'))\n",
    "source_df = source_df.sort_values([\"task_id\"])\n",
    "source_tasks = sorted(source_df[\"task_id\"].unique())\n",
    "\n",
    "assert chat_tasks == source_tasks\n",
    "\n",
    "X_source = []\n",
    "z_source = []\n",
    "\n",
    "last_task = -1\n",
    "for task in source_tasks:\n",
    "    assert task > last_task\n",
    "    emb = eval(source_df[source_df[\"task_id\"] == task][\"embedding\"].tolist()[0])\n",
    "    X_source.append(emb)\n",
    "\n",
    "    score = chat_\n",
    "    \n",
    "    last_task = task\n",
    "    \n",
    "X_source = np.array(X_source)\n",
    "X_source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7379ec3c-f8ad-4636-80f4-2952ca1aa972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_root = \"red_team\"\n",
    "chat_dir = os.path.join(output_dir, \"{}_chat\".format(chat_root))\n",
    "\n",
    "chat_df = pd.read_csv(os.path.join(chat_dir, 'google-flan-t5-xxl_predictions.csv'))\n",
    "chat_df = chat_df.sort_values([\"task_id\"])\n",
    "chat_tasks = sorted(chat_df[\"task_id\"].unique())\n",
    "\n",
    "target_df = pd.read_csv(os.path.join(chat_dir, 'sentence-transformers-multi-qa-mpnet-base-dot-v1_embeddings.csv'))\n",
    "target_df = target_df.sort_values([\"task_id\"])\n",
    "target_tasks = sorted(target_df[\"task_id\"].unique())\n",
    "\n",
    "assert chat_tasks == target_tasks\n",
    "\n",
    "X_target = []\n",
    "\n",
    "last_task = -1\n",
    "for task in target_tasks:\n",
    "    assert task > last_task\n",
    "    emb = eval(target_df[target_df[\"task_id\"] == task][\"embedding\"].tolist()[0])\n",
    "    X_target.append(emb)\n",
    "    last_task = task\n",
    "    \n",
    "X_target = np.array(X_target)\n",
    "X_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f13b75b-0935-4aa6-bff3-386842084c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dists for source and target\n",
    "# Train classifier\n",
    "# Calculate importance weights\n",
    "# Get bin boundaries\n",
    "# Run below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57ea7d30-c311-45e7-b293-86610922d926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 768) (18000, 768)\n",
      "(2000,) (18000,)\n",
      "(2000,) (18000,)\n"
     ]
    }
   ],
   "source": [
    "def unison_shuffled_copies(a, b, c):\n",
    "    assert len(a) == len(b) and len(b) == len(c)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p], c[p]\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# n_source = X_source.shape[0]\n",
    "# n_target = X_target.shape[0]\n",
    "\n",
    "n_source = 10000\n",
    "n_target = 10000\n",
    "\n",
    "X_source = np.random.normal(0.5,0.25,(n_source, 768))\n",
    "X_target = np.random.normal(0.5,0.25,(n_target, 768))\n",
    "\n",
    "y_source = np.ones(n_source)\n",
    "y_target = np.zeros(n_target)\n",
    "\n",
    "z_source = np.clip(np.random.normal(0.25,0.25,n_source), 0, 1)\n",
    "z_target = np.clip(np.random.normal(0.75,0.25,n_target), 0, 1)\n",
    "\n",
    "# plt.scatter(X_source[:, 0], X_source[:, 1], alpha=0.5, label=\"source\")\n",
    "# plt.scatter(X_target[:, 0], X_target[:, 1], alpha=0.5, label=\"target\")\n",
    "# plt.scatter(target_data[:, 0], target_data[:, 1], alpha=0.5, label=\"target\")\n",
    "# plt.show()\n",
    "\n",
    "X = np.concatenate([X_source, X_target])\n",
    "y = np.concatenate([y_source, y_target])\n",
    "z = np.concatenate([z_source, z_target])\n",
    "\n",
    "n_train = int(n_source*0.2)\n",
    "X, y, z = unison_shuffled_copies(X, y, z)\n",
    "X_train, X_test = X[:n_train], X[n_train:]\n",
    "y_train, y_test = y[:n_train], y[n_train:]\n",
    "z_train, z_test = z[:n_train], z[n_train:]\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "print(z_train.shape, z_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "412891b7-4060-4b1f-bf9c-af2b80ecc160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.78838618\n",
      "Iteration 2, loss = 0.71264593\n",
      "Iteration 3, loss = 0.70625763\n",
      "Iteration 4, loss = 0.69497843\n",
      "Iteration 5, loss = 0.69161089\n",
      "Iteration 6, loss = 0.68753725\n",
      "Iteration 7, loss = 0.68517506\n",
      "Iteration 8, loss = 0.68147992\n",
      "Iteration 9, loss = 0.67749718\n",
      "Iteration 10, loss = 0.67745259\n",
      "Iteration 11, loss = 0.67090195\n",
      "Iteration 12, loss = 0.66687047\n",
      "Iteration 13, loss = 0.66573716\n",
      "Iteration 14, loss = 0.65951243\n",
      "Iteration 15, loss = 0.65500136\n",
      "Iteration 16, loss = 0.64872327\n",
      "Iteration 17, loss = 0.64372259\n",
      "Iteration 18, loss = 0.64322293\n",
      "Iteration 19, loss = 0.64805833\n",
      "Iteration 20, loss = 0.64273659\n",
      "Iteration 21, loss = 0.63686194\n",
      "Iteration 22, loss = 0.62464118\n",
      "Iteration 23, loss = 0.61537552\n",
      "Iteration 24, loss = 0.61026349\n",
      "Iteration 25, loss = 0.60576389\n",
      "Iteration 26, loss = 0.59770156\n",
      "Iteration 27, loss = 0.59391218\n",
      "Iteration 28, loss = 0.59001136\n",
      "Iteration 29, loss = 0.59560978\n",
      "Iteration 30, loss = 0.58792023\n",
      "Iteration 31, loss = 0.57852375\n",
      "Iteration 32, loss = 0.56936199\n",
      "Iteration 33, loss = 0.55984989\n",
      "Iteration 34, loss = 0.55592294\n",
      "Iteration 35, loss = 0.55845643\n",
      "Iteration 36, loss = 0.55107022\n",
      "Iteration 37, loss = 0.54923178\n",
      "Iteration 38, loss = 0.54303967\n",
      "Iteration 39, loss = 0.53260904\n",
      "Iteration 40, loss = 0.53250699\n",
      "Iteration 41, loss = 0.52206736\n",
      "Iteration 42, loss = 0.53041466\n",
      "Iteration 43, loss = 0.53480298\n",
      "Iteration 44, loss = 0.51339042\n",
      "Iteration 45, loss = 0.51066807\n",
      "Iteration 46, loss = 0.49952929\n",
      "Iteration 47, loss = 0.50373574\n",
      "Iteration 48, loss = 0.48894975\n",
      "Iteration 49, loss = 0.48799168\n",
      "Iteration 50, loss = 0.48704143\n",
      "score 0.4960555555555556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/prompt_risk/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, \n",
    "#     y, \n",
    "#     train_size = 0.25,\n",
    "#     stratify=y,\n",
    "#     random_state=1\n",
    "# )\n",
    "\n",
    "clf = MLPClassifier(random_state=1, max_iter=50, verbose=True).fit(X_train, y_train)\n",
    "\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"score\", score)\n",
    "# print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59efadfa-b25b-4ad6-8276-0fc7e644c7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9002, 768) (9002,) (9002,)\n",
      "(8998, 768) (8998,) (8998,)\n"
     ]
    }
   ],
   "source": [
    "source_data = (X_test[y_test == 1], y_test[y_test == 1], z_test[y_test == 1])\n",
    "target_data = (X_test[y_test == 0], y_test[y_test == 0], z_test[y_test == 0])\n",
    "\n",
    "# clf = MLPClassifier(random_state=1, max_iter=50, verbose=True).fit(X, y)\n",
    "\n",
    "# score = clf.score(X, y)\n",
    "# print(\"score\", score)\n",
    "# print(X.shape, X.shape)\n",
    "\n",
    "# source_data = (X[y == 1], y[y == 1])\n",
    "# target_data = (X[y == 0], y[y == 0])\n",
    "\n",
    "print(source_data[0].shape, source_data[1].shape, source_data[2].shape)\n",
    "print(target_data[0].shape, target_data[1].shape, target_data[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd3d2a77-b401-493e-b8c5-658a8cc879bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18000, 768), (18000,), (18000,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val = np.concatenate([source_data[0], target_data[0]])\n",
    "y_val = np.concatenate([source_data[1], target_data[1]])\n",
    "z_val = np.concatenate([source_data[2], target_data[2]])\n",
    "X_val.shape, y_val.shape, z_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "178bc00a-2ae5-45c1-b34d-bca64572831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 5\n",
    "delta = 0.05\n",
    "E = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38b2c02e-2a98-4097-9fa5-db4abd0d22ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## histogram binning with n_bins = 5, delta = 5.000000e-02, and E = 0.001\n",
      "[bin edges] [0.         0.69459888 1.18068726 1.87631833 3.14481509        inf]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(18000,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'## histogram binning with n_bins = {n_bins}, delta = {delta:e}, and E = {E}')\n",
    "\n",
    "bin_edges = find_bin_edges_equal_mass_src(source_data, n_bins, clf)\n",
    "assert(len(bin_edges) == n_bins+1)\n",
    "\n",
    "print('[bin edges]', bin_edges)\n",
    "\n",
    "iw = (1/clf.predict_proba(X_val)[:, 1])-1\n",
    "# iw = clf.predict_proba(X_val)[:, 1]\n",
    "iw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6639d1ba-e72a-4dcf-82c3-c3791dcdb77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin_id = 1, n_src = 1801, n_tar = 1820\n",
      "bin_id = 2, n_src = 1800, n_tar = 1789\n",
      "bin_id = 3, n_src = 1801, n_tar = 1780\n",
      "bin_id = 4, n_src = 1800, n_tar = 1776\n",
      "bin_id = 5, n_src = 1800, n_tar = 1833\n"
     ]
    }
   ],
   "source": [
    "n_src_list, n_tar_list, iw_est = [], [], []\n",
    "for i, (l, u) in enumerate(zip(bin_edges[:-1], bin_edges[1:])):\n",
    "    if i == len(bin_edges)-2:\n",
    "        idx = (iw>=l) & (iw<=u)\n",
    "    else:\n",
    "        idx = (iw>=l) & (iw<u)\n",
    "    label_i = y_val[idx]\n",
    "    n_src = np.sum(label_i == 1)\n",
    "    n_tar = np.sum(label_i == 0)\n",
    "\n",
    "    print(f'bin_id = {i+1}, n_src = {n_src}, n_tar = {n_tar}')\n",
    "\n",
    "    iw_est.append((l+u)/2.0)\n",
    "    n_src_list.append(n_src)\n",
    "    n_tar_list.append(n_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f8588ea-fcc6-4b25-a974-4a8cd16ee811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[src] [1801 1800 1801 1800 1800] 9002\n",
      "[tar] [1820 1789 1780 1776 1833] 8998\n",
      "[itv_rate_src] [(0.18836488709195212, 0.21214300544007075), (0.18825633367210096, 0.2120295252482702), (0.18836488709195212, 0.21214300544007075), (0.18825633367210096, 0.2120295252482702), (0.18825633367210096, 0.2120295252482702)]\n",
      "[itv_rate_tar] [(0.19051294023366067, 0.2143933973179572), (0.18714609525217568, 0.21087414232963356), (0.18616888155148315, 0.20985216587396957), (0.1857346018787794, 0.20939791661151091), (0.19192524751325887, 0.21586880962460678)]\n",
      "\n",
      "[lower] [0.88913516 0.87380421 0.8687542  0.8671784  0.89623843]\n",
      "[upper] [1.14959319 1.13146583 1.12535582 1.12358238 1.15813871]\n",
      "[mean] [1.01936417 1.00263502 0.99705501 0.99538039 1.02718857]\n",
      "[iw_max] 1.15813871483973\n",
      "\n",
      "w hat [1.00263502 1.02718857 1.00263502 ... 1.02718857 1.01936417 1.02718857]\n",
      "diffs [0.26045803 0.25766162 0.25660162 0.25640398 0.26190028]\n",
      "epsilon 0.2619002836140608 0.35483048943094003\n"
     ]
    }
   ],
   "source": [
    "iw_est, n_src_list, n_tar_list = np.array(iw_est), np.array(n_src_list), np.array(n_tar_list)\n",
    "n_src_all, n_tar_all = np.sum(n_src_list), np.sum(n_tar_list)\n",
    "print('[src]', n_src_list, n_src_all)\n",
    "print('[tar]', n_tar_list, n_tar_all)\n",
    "\n",
    "## estimate CP intervals\n",
    "itv_rate_src = [bci_clopper_pearson(k, n_src_all, delta / n_bins / 2.0) for k in n_src_list]\n",
    "itv_rate_tar = [bci_clopper_pearson(k, n_tar_all, delta / n_bins / 2.0) for k in n_tar_list]\n",
    "\n",
    "print('[itv_rate_src]', itv_rate_src)\n",
    "print('[itv_rate_tar]', itv_rate_tar)\n",
    "print()\n",
    "\n",
    "## compute iw lower/upper/mean. Note that add a small value to avoid numerical error\n",
    "iw_lower = np.array([max(0, n_tar[0] - E)/(n_src[1] + E + 1e-16) for n_src, n_tar in zip(itv_rate_src, itv_rate_tar)]) \n",
    "iw_upper = np.array([(n_tar[1] + E)/(max(0, n_src[0] - E) + 1e-16) for n_src, n_tar in zip(itv_rate_src, itv_rate_tar)])\n",
    "iw_mean = (iw_lower + iw_upper) / 2.0\n",
    "\n",
    "print('[lower]', iw_lower)\n",
    "print('[upper]', iw_upper)\n",
    "print('[mean]', iw_mean)\n",
    "print('[iw_max]', np.max(iw_upper))\n",
    "print()\n",
    "\n",
    "w_hat = np.zeros_like(y_val)\n",
    "\n",
    "for i, (l, u) in enumerate(zip(bin_edges[:-1], bin_edges[1:])):\n",
    "    if i == len(bin_edges)-2:\n",
    "        idx = (iw>=l) & (iw<=u)\n",
    "    else:\n",
    "        idx = (iw>=l) & (iw<u)\n",
    "    w_hat[idx] = iw_mean[i]\n",
    "\n",
    "print(\"w hat\", w_hat)\n",
    "\n",
    "print(\"diffs\", iw_upper - iw_lower)\n",
    "\n",
    "epsilon = np.max(iw_upper - iw_lower)\n",
    "print(\"epsilon\", epsilon, epsilon/(1-epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7d292f6-1a45-4a59-9e50-8a48666af489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9002,), (8998,), 1.0271885730326995)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_hat_source = w_hat[y_test == 1]\n",
    "w_hat_target = w_hat[y_test == 0]\n",
    "\n",
    "w_hat_max = np.max(w_hat_source)\n",
    "\n",
    "w_hat_source.shape, w_hat_target.shape, w_hat_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70335c93-2006-41b0-a216-20e4af0fde71",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = np.random.uniform(0,1,w_hat_source.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91d29382-a73b-4c6a-b3c4-ab1a76f98047",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_target = target_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6026a7f-39c2-4853-9b18-5f371aa563b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8837,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_source = source_data[2][(w_hat_source/w_hat_max) > V]\n",
    "z_source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "270efbc0-6677-42fd-81de-5f705928dfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_risk.bounds import *\n",
    "from prompt_risk.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9668520-70e6-41f3-be12-6940b2e2561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cal = z_source.shape[0]\n",
    "if n_cal not in bj_bounds:\n",
    "    b = berk_jones(n_cal, 0.05)\n",
    "    bj_bounds[n_cal] = b\n",
    "else:\n",
    "    b = bj_bounds[n_cal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd648af-dca1-48a7-be90-18f4689f8c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inflation = epsilon/(1-epsilon)\n",
    "plt.plot(b, np.sort(z_source), label=\"unshifted bound\")\n",
    "plt.plot(b+inflation, np.sort(z_source), label=\"shift bound\")\n",
    "plt.plot(np.arange(z_target)/len(z_target), np.sort(z_target), label=\"empirical\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
